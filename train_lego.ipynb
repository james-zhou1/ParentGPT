{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is designed to test the hypothesis that the order of data can significantly impact the speed at which a neural network training converges. \n",
    "\n",
    "We will be using the LEGO dataset for this experiment. The LEGO dataset is a rich source of sequential variable assignments and negations, making it an ideal testbed for our hypothesis.\n",
    "\n",
    "We will experiment with various data orderings. These include multiple random orderings, and datapoints ranked by GPT 3.5 with respect to the perceived difficulty level of the datapoint. \n",
    "\n",
    "In addition, we will also explore a scenario where we construct a hypothetical skill tree required to achieve a low loss on the dataset. The datapoints will then be sorted according to a topological sort of the skills involved. \n",
    "\n",
    "Let's begin by importing the necessary libraries and loading the GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' gamears su 0 years saithper��']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "from torchtyping import TensorType\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Define the type for the input tensor\n",
    "InputTensorType = TensorType[\"batch\", \"sequence\"]\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Example usage:\n",
    "input_tensor = torch.randint(0, 1000, (1, 10))  # Random tensor for testing. Dimensions represent [batch_size, sequence_length]\n",
    "output = model(input_tensor)  # Output tensor from the GPT2 model. Dimensions represent [batch_size, sequence_length, hidden_state]\n",
    "token_ids = input_tensor.tolist()\n",
    "# Decode the token ids to tokens using batch decode\n",
    "decoded_output = tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
